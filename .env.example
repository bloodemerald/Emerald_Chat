# Emerald Chat Configuration
# This app uses local Ollama for AI chat generation

# No environment variables are required for basic functionality
# The app will use Ollama running on http://localhost:11434 by default

# Instructions:
# 1. Install Ollama from https://ollama.ai
# 2. Run: ollama serve
# 3. Pull a vision model: ollama pull llava:7b
# 4. (Optional) Pull a text model: ollama pull llama3.2:3b
# 5. Start the app and begin chatting!

# You can customize the Ollama API URL in the app settings if needed
